{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import enchant\n",
    "from re import compile as _Re\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "n = 3\n",
    "MAX_VOCAB = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(df):\n",
    "    docs = []\n",
    "    #make docs\n",
    "    for row in df.values:\n",
    "        doc = []\n",
    "        for idx, inst in enumerate(row[2:6]):\n",
    "            if inst != inst:\n",
    "                inst = '<NAN>'\n",
    "            elif idx == 1 or idx == 2:                                       #title, body_html\n",
    "                string = inst.replace('\\n', '')\n",
    "                string = re.sub(r'[a-zA-z+]', r'', string)\n",
    "                string = re.sub(r'[^\\w\\s]', r'', string)\n",
    "                string = re.sub(r'[0-9]+', r'', string)\n",
    "                string = string.replace(' ', '')\n",
    "                string = re.sub(r'(.)', '\\g<1> ', string)\n",
    "                doc.append(string)\n",
    "            else:\n",
    "                doc.append(inst)\n",
    "            sent = ' '.join(doc)\n",
    "        docs.append(sent)\n",
    "        del sent, doc, string\n",
    "        \n",
    "    #form tfidf table    \n",
    "    cv = CountVectorizer(tokenizer = lambda x: x.split(), ngram_range = (1, n))\n",
    "    word_count_vector = cv.fit_transform(docs)\n",
    "    tfidf_trans = TfidfTransformer(smooth_idf = True,use_idf = True) \n",
    "    tfidf_trans.fit(word_count_vector)\n",
    "    wordcount = cv.transform(docs)\n",
    "    tfidf_vectors = tfidf_trans.transform(wordcount)\n",
    "    del cv, word_count_vector, tfidf_trans, wordcount\n",
    "    \n",
    "    #form id to tfidf dictionary\n",
    "#    tfidf_vectors = dict([(identity, tfidf_vectors[idx].toarray().squeeze()) for idx, identity in enumerate(df['id'])])\n",
    "#     id_to_tfidf = {}\n",
    "#     for idx, identity in enumerate(df['id']):\n",
    "#         id_to_tfidf[identity] = tfidf_vectors[idx].toarray().squeeze()\n",
    "#     del tfidf_vectors\n",
    "    return tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw data...\n",
      "Forming tfidf vectors...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-142eb26b8298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#get tfidf dictionary for product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Forming tfidf vectors...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtfidf_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving tfidf vectors...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./idx_to_tfidf.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0ecfe33d8048>\u001b[0m in \u001b[0;36mtfidf\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m                                       \u001b[0;31m#title, body_html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[a-zA-z+]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\w\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[0-9]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def temp_mapping(t):                                        #map temperature categories to words\n",
    "    mapped = []\n",
    "    for temp_type in t:\n",
    "        if temp_type == 1:\n",
    "            mapped.append('room temperature')\n",
    "        elif temp_type == 2:\n",
    "            mapped.append('cold')\n",
    "        else:\n",
    "            mapped.append('frozen')\n",
    "    return mapped\n",
    "def get_emb_weight(path):                                        #form the glove file into vector\n",
    "    emb_dict = {}\n",
    "    with open(path, 'r', encoding='utf-8') as emb_file:\n",
    "        for line in emb_file:\n",
    "            vec = line.split()\n",
    "            word = vec[0]\n",
    "            emb_dict[word] = np.array((vec[1:]), dtype = float)\n",
    "    return emb_dict\n",
    "if __name__ == '__main__':\n",
    "    data_name = 'undup_seo'\n",
    "    save_path = './embedded/'\n",
    "    \n",
    "    #read in raw data\n",
    "    print('Reading raw data...')\n",
    "    with open('{}.pickle'.format(data_name), 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "    df = df.drop(df[df['price'] == 0].index)\n",
    "    df = df.drop(df[df['price'] > 1000000].index)\n",
    "    \n",
    "    #get tfidf dictionary for product\n",
    "    print('Forming tfidf vectors...')\n",
    "    tfidf_vectors = tfidf(df)\n",
    "    print('Saving tfidf vectors...')\n",
    "    with open('./idx_to_tfidf.pickle', 'wb') as out:\n",
    "        pickle.dump(tfidf_vectors, out)\n",
    "    \n",
    "    #append price info to product feature vector\n",
    "    print('Concatenating price information...')\n",
    "    for idx, inst in enumerate(df['price']):\n",
    "        if inst > 1.7976931348623157e+308:\n",
    "            df['price'][idx] = 1\n",
    "    tfidf_vectors = np.append(tfidf_vectors.toarray(), np.array(df['price'])[:, np.newaxis], axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 20):\n",
    "    kmeans = KMeans(n_clusters = i, max_iter = 300, n_init = 10, init = 'k-means++')\n",
    "    kmeans.fit(tfidf_vectors)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    print()\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Fitting K-Means to the dataset\n",
    "kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-00ebf5ef3a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msplit_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# seperate chinese and english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m            \u001b[0;31m#is english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/site-packages/enchant/__init__.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can't check spelling of empty string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_this\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/site-packages/enchant/_enchant.py\u001b[0m in \u001b[0;36mdict_check\u001b[0;34m(dict, word)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdict_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict_check1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# chinese_splitter = _Re( '(?s)((?:[\\ud800-\\udbff][\\udc00-\\udfff])|.)' ).split\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "# doc = []\n",
    "# for row in df.values:\n",
    "#     r = ''\n",
    "#     if(row[1] == row[1]):\n",
    "#         fixed_str = fix_str(row[1])\n",
    "#         split_str = fixed_str.split()    # seperate chinese and english\n",
    "#         for text in split_str:\n",
    "#             if d.check(text):            #is english\n",
    "#                 r += text\n",
    "#                 r += ' '\n",
    "#             else:\n",
    "#                 for c in chinese_splitter(text):\n",
    "#                     r += c\n",
    "#                     r += ' '\n",
    "            \n",
    "#     doc.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     data_name = 'big_query_results'\n",
    "#     df = pd.read_csv('./{}.csv'.format(data_name))\n",
    "#     save_path = './embedded/'\n",
    "    \n",
    "#     title = df['title']\n",
    "#     body_html = df['body_html']\n",
    "#     price = df['price']\n",
    "#     temperature_type = df['temperature_type']\n",
    "#     t_type = temp_mapping(temperature_type)\n",
    "    \n",
    "#     sampled = {'title' : title, 'body_html' : body_html, 'price' : price, 'temp' : t_type}\n",
    "#     sampled = pd.DataFrame(sampled)\n",
    "#     train, test = train_test_split(sampled, test_size = 0.1, random_state = 0)\n",
    "#     with open(save_path + 'sampled_train.pickle', 'wb') as out:\n",
    "#         pickle.dump(train, out)\n",
    "#     with open(save_path + 'sampled_test.pickle', 'wb') as out:\n",
    "#         pickle.dump(test, out)\n",
    "\n",
    "# def ngrams_with_tfidf(df):\n",
    "#     docs = []\n",
    "#     tf = defaultdict(int)\n",
    "#     for idx, row in enumerate(df.values):\n",
    "#         string = re.sub(r'(.)', '\\g<1> ', row[4])            #pick out body_html\n",
    "#         docs.append(string)\n",
    "#     corpus = ' '.join(docs)\n",
    "#     n_gram = ngrams(corpus, n)\n",
    "#     term_freq = collections.Counter(n_gram)\n",
    "#     term_freq = term_freq.most_common(WORD_FREQ)\n",
    "    \n",
    "#     #tf calculation\n",
    "#     for combi in term_freq:                \n",
    "#         w_pair, w_freq = combi[0], combi[1]\n",
    "#         tf[w_pair] = w_freq\n",
    "        \n",
    "#     #idf calculation\n",
    "#     idf = defaultdict(int)\n",
    "#     for sent in docs:\n",
    "#         seen = set()\n",
    "#         words = sent.split()\n",
    "#         for idx in range(len(words) - n + 1):\n",
    "#             word_combi = tuple(words[idx : idx + n])\n",
    "#             if word_combi not in seen:\n",
    "#                 seen.union(word_combi)\n",
    "#                 idf[word_combi] += 1\n",
    "    \n",
    "#     #form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
